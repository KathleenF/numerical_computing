\lab{Algorithm}{Compressed Sensing}{Compressed Sensing}
\label{Ch:CS}

\objective{Learn About Techniques in Compressed Sensing.}


One of the more important and fundamental problems in mathematics and science is solving a system of linear equations
\[
Ax = b.
\]
Depending on the properties of the matrix $A$ (such as its dimensions and rank), there may be exactly one
solution, infinitely many solutions, or no solution at all. 

In the case where $A$ is a square invertible matrix, there is of course one unique solution, given by
$A^{-1}b$. There are various computational methods for inverting $A$, and we have studied many of them previously.

When $b$ does not lie in the range of $A$, there is no exact solution. We can still hope to find approximate solutions,
and techniques such as least squares and least absolute deviations provide ways to do this.

The final case is when there are infinitely many vectors $x$ that satisfy $Ax = b$. How do you we decide which vector
to choose? A common approach is to choose a vector $x$ of minimal norm satisfying $Ax = b$. This can be stated as 
an optimization problem:
\begin{align*}
\text{minimize}\qquad &\|x\|\\
\text{subject to} \qquad &Ax = b.
\end{align*}

When we use the standard Euclidean $2$-norm, the problem is equivalent to the quadratic program
\begin{align*}
\text{minimize}\qquad &x^Tx\\
\text{subject to} \qquad &Ax = b,
\end{align*}
which we can solve using an iterative procedure. Alternatively, the solution is given directly by $A^\dagger b$,
where $A^\dagger$ is the Moore-Penrose pseudoinverse of $A$. 

If instead of the $2$-norm we use the $1$-norm, our problem can be restated as a linear program, and solved
efficiently using the Simplex Algorithm or an Interior Point method. Of course we can use any norm whatsoever, but
finding the solution may be much more difficult.

The basic problem in the field of Compressed Sensing is to solve the under-determined system of euqations
$Ax = b$, where there are infinitely many possibilities. The crucial idea that Compressed Sensing brings
to the table is the concept of \emph{sparsity}, which we address now.

\section*{Sparsity and the $l_0$ Pseudonorm}
\emph{Sparsity} is a property of vectors in $\mathbb{R}^n$ related to how compactly and concisely they can
be represented in a given basis. Stated more concretely, the sparsity of a vector $x$ (expressed in some given
basis) refers to how many nonzero entries are in $x$. A vector having at most $k$ nonzero entries is said to be
$k$-sparse. This concept can be extended to time series (such as an audio signal) and to images. Figure REF shows
both sparse and non-sparse examples of these.

INCLUDE SPARSE AND DENSE SIGNAL, SPARE IMAGE AND LENA IMAGE

As a convenient way of measuring sparsity, we define the so-called ``$l_0$" pseudonorm, notated $\|\cdot\|_0$,
that simply counts the number of nonzero entries in a vector. For example, if we have
\[
x = [1\,\,\,0\,\,\,0\,\,\,-4\,\,\,0]^T
\]
and
\[
y = [.5\,\,\,.2\,\,\,-.01\,\,\,3\,\,\,0]^T,
\]
we then have
\[
\|x\|_0 = 2
\]
and
\[
\|y\|_0 = 4.
\]
Despite our choice of notation, $\|\cdot\|_0$ is not truly a norm (which properties does it fail to satisfy?).
Keep this in mind, even if we refer to it at the $l_0$ norm. 

As mentioned earlier, sparsity is of central importance in Compressed Sensing, for it provides a way for us to 
select from the infinite possibilities a single vector $x$ satisfying $Ax = b$. In particular, we require $x$ to 
be as sparse as possible, i.e. to have minimal $l_0$ norm. Stated explicitly as an optimization problem, Compressed
Sensing boils down 
\begin{align*}
\text{minimize}\qquad &\|x\|_0\\
\text{subject to} \qquad &Ax = b.
\end{align*}


\section*{Shannon-Nyquist}

If you want to reconstruct a signal perfectly how many samples do you need? According to the  Shannon-Nyquist thereom you need twice as many as your highest frequency.


\begin{problem}
Yea
\end{problem}


\section*{Compressed Sensing}


Compressed Sensing is a new technique for reconstructing sparse signals using a relatively small number of measurements. Specifically, Compressed Sensing allows for the exact reconstruction of a sparse $m-$dimensional signal with $k$ non-zero components with $O(k \log (m))$ measurements. This is a tremendous result, particularly because it improves upon the Shannon-Nyquist criteria \emph{exponentially}! This technique was first developed in 2005 and 2006, and since has been applied in a variety of fields, including image and video processing , MRI, machine learning and communications.

We will explain the basic idea behind Compressed Sensing using a simple example presented in [Hayes 2009]


\hangindent = .7cm
\setlength{\parindent}{.7cm}
\textbf{Counterfeit Coin Problem:} Given $2^n$ coins, with one lighter than the rest, how many measurements (on a balance) are needed to identify the lighter coin?
\setlength{\parindent}{0cm}

The answer, as you might have deduced, is $\log(n)$ (For example in the case of 8 coins you measure $\{1,2,3,4\}$, $\{1,2,5,6\}$ and $\{1,3,5,7\}$. In this setting it's quite obvious that individually measuring each coin is highly ineffective. You can probably further deduce that identifying $k$ lighter coins would take $O(k \log(n))$ measurements.

Several recent results say that we can reconstruct more general sparse signals using a similar technique. Specifically we have the following result:

\hangindent = .7cm
\setlength{\parindent}{.7cm}
\textbf{Compressed Sensing:} Suppose that we have $x \in \R^n$  sufficiently sparse and an $m \times n$ matrix $A$ $(m < n)$ satisfying certain properties (to be described later), with $Ax = b$. Then the solution of the optimization problem:
\begin{equation*}
\begin{aligned}
& \underset{x}{\text{minimize}}
& & \|x\|_0\\
& \text{subject to}
& & Ax = b.
\end{aligned}
\end{equation*}
 will yield $x$.
\setlength{\parindent}{0cm}

One important technical detail is the matrix $A$.  It must have what is called the \emph{Restricted Isometry Principle}. This property is rather technical, but it is generally satisfied by a random matrix of appropriate size. The ``randomness'' of the matrix is not very restrictive, and most random matrices based upon standard distributions satisfy this property, including bernoulli and gaussian.

We note that the signal need not be sparse in the domain in which we are measuring. The signal may be sparse in some appropriate transform domain (i.e. Fourier or wavelet). This makes this approach very appealing in a variety of applications, particularly to graphics and image processing. When the measurements are in a different basis than the matrix that the signal is sparse in than you can change the form to $ADx = b$. Where is $D$ is an $n$ by $n$ change of basis matrix and $A$ in a random matrix as defined above. Some basis, such as the fourier transformation, sastify the \emph{Restricted Isometry Principle}. So you can sample in the time domain and reconstuct the sparse signal in the fourier basis.

\section*{The $L_0$ norm}

The $L_0$ norm is defined as follows: For $x \in \mathbb{R}^n$, $\|x\|_0=\sum_{k=1}^n{x_k^0}$ or the number of non-zero elements to x. This problem is, in general, NP-Hard. However, the groundbreaking results  guarantee that, given a sufficiently sparse signal, with overwhelming probability we can replace $l_0$ with $l_1$ and acheive the same result.
TO DO see picture.
The $l_1$ minimization problem is a convex problem, and thus is generally amicable to computation.

\section*{How to solve it and code examples}


\begin{problem}
How to, do a problem
\end{problem}

\begin{problem}
Use this file. It is sparse.
\end{problem}



