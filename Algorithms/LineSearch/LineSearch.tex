\lab{Algorithms}{Line-Search Algorithms}{Line-Search Algorithms}
\objective{Investigate various Line-Search algorithms for numerical optimization.}

\section*{Overview of Line Search Algorithms}
Imagine you are out hiking on a mountain, and you lose track of the trail. Thick fog 
gathers around, reducing visibility to just a couple of feet. You decide it is time 
to head back home, which is located in the valley located near the base of the mountain.
How can you find your way back with such limited visibility? The obvious way might be to
pick a direction that leads downhill, and follow that direction as far as you can, or
until it starts leading upward again. Then you might choose another downhill direction,
and take that as far as you can, repeating the process. By always choosing a downhill 
direction, you hope to eventually make it back to the bottom of the valley, where you live.

This is the basic approach of line search algorithms for numerical optimization.
Suppose we have a real-valued function $f$ that we wish to minimize. Our goal is to find the
point $x^*$ in the domain of $f$ such that $f(x^*)$ is the smallest value in the range of
$f$. For some functions, we can use techniques from calculus to analytically obtain this 
minimizer. However, in practical applications, this is often impossible, especially when
we need a system that works for a wide class of functions. A line search algorithm starts with
an initial guess at the minimizer, call it $x_0$, and iteratively produces a sequence of
points $x_1, x_2, x_3, \ldots$ that hopefully converge to the minimizer $x^*$. The basic 
iteration to move from $x_k$ to $x_{k+1}$ involves two steps: first, choosing a search direction $p_k$
in which to proceed from the current point, and second, specifying a step size $\alpha_k$ to travel
in this direction. The next point is determined by the formula
$$
x_{k+1} = x_k + \alpha_kp_k.
$$
This procedure is called a line search because at each iteration, we are simply examining the
function in a particular linear direction. The choice of the step size $\alpha_k$ is often 
chosen by solving a one-dimensional optimization problem in the given direction. In this lab,
we will discuss various approaches to choosing the step size and the search direction.

%Line search procedures are an integral part of many nonlinear optimization techniques.
%In some sense, they represent the simplest nontrivial case in general optimization, as
%we only have to worry about one parameter. And yet far more sophisticated optimization
%algorithms really crucially on the effectiveness and efficiency of line searches, since
%higher-dimensional problems are often broken down into one-dimensional optimizations.
%There are many different line search methods, and their effectiveness depends very much
%on the nature of the optimization problem. Although the line search procedure is often
%only a subroutine of the optimization algorithm at hand, understanding the basics of
%the line search is necessary for understanding the robustness of the entire algorithm.
\section*{Optimizing Functions on the Real Numbers}
\subsection*{Derivative versus Derivative-Free Methods}
As you have seen in calculus classes, the derivative of a function gives information
about how the value of the function changes at each point, and can be used to determine
local optima. However, not all objective functions are differentiable, so we need other
techniques at our disposal. Line search methods may be broadly separated into two groups
based on whether they use the derivative of the objective function. We discuss two
simple examples to illustrate this distinction.

\subsection*{Golden Section Search}
This method is appropriate when minimizing a real-valued function on the reals over a
closed interval. The function must further satisfy the \emph{unimodal} property, i.e.
it has just one local minimum, and is monotonic to the left and right of the minimum.
The goal, of course, find the
global minimum. We do this by making a sequence of guesses that we hope will converge
quickly to the minimum. Although we may not end up with the exact minimum, this method
will allow us to pin down the true minimum within an interval of any given width in a
finite number of steps.

For the Golden Section Search, each step consists of evaluating the function at two
points within the current interval, comparing these values, and then reducing the size
of the interval for the next step. Let us consider typical step in the algorithm. At
the outset, we have our function $f$ and a closed interval $[a, b]$ over which we seek
to minimize $f$. Choose two points $a'$ and $b'$ within the interval, and assume that
$a' < b'$. Now calculate $f(a')$ and $f(b')$, and assume that $f(a') \geq f(b')$.
Because of the unimodal condition, we now know that the minimizer must be in the
interval $[a', b]$, for otherwise the function $f$ would have a local minimum in
both $[a, a']$ and $[a', b]$. In the next step, we repeat the process over the interval
$[a', b]$. If instead we had $f(b') \geq f(a')$, then we choose the interval $[a, b']$
for the next step, and if the two values are equal, then it does not matter which
interval is chosen.

We now have the basic description of the algorithm, but how do we choose the two test
points $a'$ and $b'$? There is in fact an optimal choice, which reduces the amount of
work we have to do. Given an interval $[a, b]$, choose $a'$ and $b'$ satisfying
\begin{align*}
a' &= a + \rho(b - a) \\
b' &= a + (1 - \rho)(b - a),
\end{align*}
where $\rho = \frac{1}{2}(3 - \sqrt{5}) \approx 0.382$. By choosing these particular
points, we need to only evaluate the function at one additional point in the next step.
To demonstrate this fact, the reader may verify that, within the interval $[a, b']$,
the point $a'$ already satisfies the equation
\begin{equation*}
a' = a + (1 - \rho)(b' - a),
\end{equation*}
and so we need only evaluate the function at the point $c$ satisfying
\begin{equation*}
c = a + \rho(b' - a).
\end{equation*}
(The constant $\rho$ is not difficult to derive, and is related to the famous Golden Ratio, hence the name of this algorithm.)

At each step, the interval is reduced by a factor of $1-\rho$, which means that after
$n$ steps, we have pinned down the minimizer to within an interval approximately
$(0.61803)^n$ times the length of the original interval. Note that this convergence is
independent of the objective function.

\begin{problem}
Implement Golden Section Search as described above. Use this to minimize $\exp{x} - 4x$
on the interval $\lbrack 0, 3 \rbrack$. How many steps do you need to take to get
within $.001$ of the true minimizer? Check that with the sentence preceding this
problem.
\end{problem}

\subsection*{One-Dimensional Newton's Method}
To use this method, we need a real-valued function of a real variable that is twice
differentiable. The idea is to approximate the function with a quadratic polynomial and
then solve the trivial problem of minimizing the polynomial. Doing so in an iterative
manner can lead us to the actual minimizer. Let $f$ be a function satisfying the
appropriate conditions, and let us make an initial guess, $x_0$. The relevant quadratic
approximation to $f$ is
\begin{equation*}
q(x) = f(x_0) + f'(x_0)(x-x_0) + \frac{1}{2}f''(x_0)(x-x_0)^2,
\end{equation*}
or just the second-degree Taylor polynomial for $f$ centered at $x_0$. The minimum
for this quadratic function is easily found by solving $q'(x) = 0$, and we take the
obtained $x$-value as our new approximation. The formula for the $(n+1)$-th
approximation, which the reader can verify, is
\begin{equation*}
x_{n+1} = x_n - \frac{f'(x_n)}{f''(x_n)}.
\end{equation*}

As is typical with optimization algorithms, Newton's Method generates a sequence of
points or successive approximations to the minimizer. However, the convergence
properties of this sequence depend heavily on the initial guess $x_0$ and the function
$f$. Roughly speaking, if $x_0$ is sufficiently close to the actual minimizer, and if
$f$ is well-approximated by parabolas, then one can expect the sequence to converge
quickly. However, there are cases when the sequence converges slowly or not at all.
See Figure \ref{linesearch:newton}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{newton.pdf}
\caption{The results of Newton's Method using two
different initial guess. The global minimizer was
correctly found with initial guess of 1. However,
an initial guess of 4 led to only a local minimum.}
\label{linesearch:newton}
\end{figure}

\begin{problem}
Implement Newton's Method as described above. Write a function called \li{newton1d} that
accepts a function object for the objective function, the first derivative, and the
second derivative, a real number giving the initial point, and an integer giving the number
of iterations. The function should perform Newton's method for the specified number of iterations
and return the approximated minimizer.

Use this function to again minimize $\exp{x} - 4x$
with an initial guess of $x_0 = 0$. Run the algorithm for as many steps as used for the
Golden Section Search. Which method gives the better result?
\end{problem}

\section*{General Line Search Methods}
\subsection*{Step Size Calculation}
We now examine Line Search methods in more generality. Given a differentiable function
$f : \mathbb{R}^n \rightarrow \mathbb{R}$ that we wish to minimize, and assuming that
we already have a current point $x_k$ and direction $p_k$ in which to search, how do we 
choose our step size $\alpha_k$? A common approach involves the \emph{Wolfe conditions}:

\begin{align*}
&f(x_k + \alpha_kp_k) \leq f(x_k) + c_1\alpha_k\nabla f_k^Tp_k, &(0 < c_1 < 1)
\\ &\nabla f(x_k + \alpha_kp_k)^Tp_k \geq c_2\nabla f_k^Tp_k, &(c_1 < c_2 < 1)
\end{align*}

Here, we use the shorthand notation $\nabla f_k$ to
mean the gradient of $f$ evaluated at the point $x_k$. The search direction $p_k$ is
often required to satisfy $p_k^T \nabla f_k < 0$, in which case it is called a
\emph{descent direction}, since in this case the function is guaranteed to decrease in
this direction. Generally speaking, choosing a step size $\alpha_k$ satisfying these conditions
ensures that we achieve sufficient decrease in the function and also that we do not
terminate the search at a point of steep decrease (since then we could achieve even
better results by choosing a slightly larger step size). The first condition is known
as the \emph{Armijo} condition. 

Finding such a step size satisfying these conditions is not always an easy task, however.
One simple approach, known as \emph{backtracking}, starts with an initial step size
$\alpha$, and repeatedly scales it down until the Armijo condition is satisfied.
That is, choose $\alpha >0, \rho \in (0, 1), c\in (0, 1)$, and while
$$
f(x_k + \alpha p_k) > f(x_k) + c\alpha\nabla f_k^Tp_k,
$$
set $\alpha := \rho\alpha$. Once the loop terminates, set $\alpha_k = \alpha$. Note that the value
$\nabla f_k^Tp_k$ remains fixed for the duration of the backtracking algorithm, and hence need only 
be calculated once at the beginning.
%The second of the two conditions can be replaced by
%\begin{equation*}
%| \nabla f(x_k + \alpha_kp_k)^Tp_k| \leq c_2 | \nabla f_k^Tp_k|,
%\end{equation*}
%and in this case we have the \emph{strong Wolfe conditions}.
%
%The \emph{Goldstein conditions} for choosing step size:
%\begin{equation*}
%f(x_k) + (1-c)\alpha_k\nabla f_k^Tp_k \leq f(x_k + \alpha_kp_k) \leq f(x_k) +
%c\alpha_k\nabla f_k^Tp_k,
%\end{equation*}
%where $0 < c < 0.5$. Similar to the Wolfe conditions, the Goldstein conditions ensure
%sufficient decrease in the function and prevent the step size from being too small. In
%the area of quasi-Newton optimization methods, however, the Wolfe conditions are
%preferred.

\begin{problem}
Implement this backtracking algorithm. Write a function \li{backtracking} that takes a function
object giving the objective function, a real number giving the value of $\nabla f_k^Tp_k$, the 
current point $x_k$, the search direction $p_k$, an initial guess for $\alpha$, the value for
$\rho$, and the value for $c$. Return the computed step size.
\end{problem}

\subsection*{Choosing a Search Direction}
There are many different ways to choose a search direction $p_k$. As noted earlier, it is usually
a requirement to choose a descent direction. We will compare two methods, both using derivative
information about the function.

\emph{Gradient Descent.} Recall that the gradient of a function at a given point gives the direction
in which the function is increasing fastest. Thus, the negative of the gradient points in the direction
of fastest decrease. In the method of Gradient Descent, we choose our search direction to be
this direction of steepest descent, that is,
$$
p_k = -\nabla f(x_k).
$$
This is a very natural choice, since we see to be approaching the minimum value as fast as possible.
However, depending on the nature of the objective function, convergence may be slow. See Figure 
\ref{linesearch:comparison}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{comparison.pdf}
\caption{Paths generated by Gradient Descent (green) and Newton's Method (blue).
Note that the Newton path takes a more direct route toward the minimizer (located
at the origin).
}
\label{linesearch:comparison}
\end{figure}

\emph{Newton's Method.} We now generalize the one-dimensional Newton's method presented above.
We use both the gradient and the Hessian matrix (which gives information on the curvature of the 
function at a given point) to choose a search direction. This is more computationally intensive,
but it leads to very fast convergence in many cases. See Figure
\ref{linesearch:comparison}. Our search direction is
$$
p_k = -\nabla^2 f_k^{-1} \nabla f_k,
$$
where $\nabla^2 f_k^{-1}$ is the inverse of the Hessian matrix of $f$ at the point $x_k$. 
In other words, $p_k$ is the solution to the linear system
$$
\nabla^2 f_k p_k = -\nabla f_k.
$$

\begin{problem}
Write a function \li{gradientDescent} that implements the Gradient Descent algorithm.
Write another function \li{newtonsMethod} that implements the general Newton's Method
described above. In each function, you should call your backtracking function with 
values $\alpha = 1, \rho = .9$, and $c = 10^{-4}$. 
\end{problem}

